{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing, or NLP for short\n",
    "\n",
    " focuses on analyzing text and speech data. \n",
    "\n",
    " Application:\n",
    " - Sentiment Analysis\n",
    "    - The food was amaing : happy\n",
    "    - The room was dirty: not happy\n",
    " - Dictation\n",
    " - Translation\n",
    " - Voice Assistants\n",
    " - Smart speakers\n",
    " - Smart home devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otline\n",
    "- 1\n",
    "    - Tokenization: Numerical format to input to NN\n",
    "    - Embidings: \n",
    "- 2\n",
    "    - RNN\n",
    "    - Text Generaation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokentizatoin\n",
    "- Tokinize an input\n",
    "- Pad sequenceis\n",
    "- Out of vocabulary words\n",
    "- Use on Read Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>OOV Token</b>\n",
    "\n",
    "Out of Vocabulary\n",
    "\n",
    "- We want to convert sentence into sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'My favorite food is ice cream',\n",
    "    'do you like ice cream too?',\n",
    "    'My dog likes ice cream!',\n",
    "    \"your favorite flavor of icecream is chocolate\",\n",
    "    \"chocolate isn't good for dogs\",\n",
    "    \"your dog, your cat, and your parrot prefer broccoli\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='OOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6, 10, 7, 3, 4],\n",
       " [11, 12, 13, 3, 4, 14],\n",
       " [5, 8, 15, 3, 4],\n",
       " [2, 6, 16, 17, 18, 7, 9],\n",
       " [9, 19, 20, 21, 22],\n",
       " [2, 8, 2, 23, 24, 2, 25, 26, 27]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sequences will either rquire padding or truncuate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  5,  6, 10,  7,  3,  4],\n",
       "       [ 0,  0,  0, 11, 12, 13,  3,  4, 14],\n",
       "       [ 0,  0,  0,  0,  5,  8, 15,  3,  4],\n",
       "       [ 0,  0,  2,  6, 16, 17, 18,  7,  9],\n",
       "       [ 0,  0,  0,  0,  9, 19, 20, 21, 22],\n",
       "       [ 2,  8,  2, 23, 24,  2, 25, 26, 27]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Embidings</b>\n",
    "- Transform into embidings\n",
    "- Develop a sentimnet model\n",
    "- visualize embiding spaces\n",
    "- tweak:\n",
    "\n",
    "    - Data and preprocessing-based approaches\n",
    "        - More data\n",
    "        - Adjusting vocabulary size (make sure to consider the overall size of the corpus!)\n",
    "        - Adjusting sequence length (more or less padding or truncation)\n",
    "        - Whether to pad or truncate pre or post (usually less of an effect than the others)\n",
    "    - Model-based approaches\n",
    "        - Adjust the number of embedding dimensions\n",
    "        - Changing use of Flatten vs. GlobalAveragePooling1D\n",
    "        - Considering other layers like Dropout\n",
    "        - Adjusting the number of nodes in intermediate fully-connected layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are clusters of vectors in multi-dimensional space, each vector represbets a given word in that space.\n",
    "- useful for sentiment analysis models, \n",
    "\n",
    "An example of a post-training embedding projection, with clear distinctions between positive and negative sentiments.\n",
    "![Alt text](explanation_images/sphereized-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I can use a flatten layer or use GlobalAveragePooling1D for a little additional computation that sometimes creates better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.layers.core.embedding.Embedding"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs\n",
    "\n",
    "- Recurrent: recurres often or repeateadly\n",
    "- Temporal Dependices: dependices over the time.\n",
    "- For example predicting the word\n",
    "- Words that affect each other can be close together\n",
    "\n",
    "- The vanishing gradient problem\n",
    "\n",
    "- Memory (state): referring to the output of the hidden layer that will be fed to the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>GRUs</b>\n",
    "\n",
    "Gated Recurrent Units, or GRUs, have “update” and “reset” gates. These gates decide what to keep and what to throw away. They do not have a “cell state” like LSTMs do.\n",
    "\n",
    "The code for these is very similar to an LSTM, where the GRU layer is wrapped in a Bidirectional layer.\n",
    "\n",
    "Functionality: Utilizes “update” and “reset” gates, where the “update” gate determines updates to the existing stored knowledge, and the reset gate determines how much to forget in the existing stored knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.rnn.bidirectional.Bidirectional at 0x1e53b6d5c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A bidirectional GRU layer with 32 nodes\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LSTM</b>\n",
    "\n",
    "Utilizes “forget” and “learn” gates that feed to “remember” and “use” gates, where remembering is for further storage for the next input, and using is for generating the current output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Convolution</b>\n",
    "\n",
    "Utilizes “filters” that can slide over multiple words at a time and extract features from those sequences of words. Can be used for purposes other than a recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Text Generation</b>\n",
    "\n",
    "Text generation can be done through simply predicting the next most likely word, given an input sequence. This can be done over and over by feeding the original input sequence, plus the newly predicted end word, as the next input sequence to the model. As such, the full output generated from a very short original input can effectively go on however long you want it to be.\n",
    "\n",
    "- Optimization\n",
    "    - Using more data?\n",
    "        - need to consider memory and output size constraints\n",
    "        - Also consider using only top-k most common words\n",
    "    - Know the data - songs have many more words than a Tweet\n",
    "    - Keep tuning your model\n",
    "        - Add/subtract from layer sizes or embedding dimensions\n",
    "    - Use np.random.choice with the probabilities for more variance in predicted outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
